% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/StackedLearner_recombine.R
\name{resampleStackedLearnerAgain}
\alias{resampleStackedLearnerAgain}
\title{Rerun a alreday done outer resample for a StackedLearner again with new settings.
Instead of computing a whole new resampling procedure just use \code{resampleStackedLearnerAgain}. 
\code{resampleStackedLearnerAgain} reuses the already done work in a \code{ResampleResult}, i.e. 
reuse fitted base models (needed for level 1 test data) and reuse level 1 training data. 
Note: This function does not support resample objects with single broken base models (no error 
handling implemented). Moreover models need to present (i.e. save.preds = TRUE in 
\code{makeStackedLearner}). When using \code{save.on.disc = TRUE} in makeStackedLearner 
resampling procedures with holdout are allowed only (model names are not unique 
regarding CV fold number).
This function does four things internally to obtain the new predictions.
\describe{
\item{1.}{Extract level 1 train data.}
\item{2.}{Fit new super learner or apply new ensemble selection setting using level 1 train data.}
\item{3.}{Use saved base models on test data to predict level 1 test data}
\item{4.}{Apply model from (2) on level 1 test data from (3) to obtain final prediction}
}
For method stack.cv \code{super.learner} and \code{use.feat} need to be set. 
For \code{hill.climb} \code{parset} need to be set. 
Method \code{average} is not supported (use no inner resampling).}
\usage{
resampleStackedLearnerAgain(id = NULL, obj, task, measures = NULL,
  super.learner = NULL, use.feat = NULL, parset = NULL)
}
\arguments{
\item{id}{[\code{character(1)}]\cr Unique ID for object}

\item{obj}{[\code{ResampleResult}]\cr Object using \code{StackedLearner} as learner.}

\item{task}{[\code{\link{Task}}]\cr
The task.}

\item{measures}{[\code{\link{Measure}} | list of \code{\link{Measure}}]\cr
Performance measure(s) to evaluate.
Default is the default measure for the task, see here \code{\link{getDefaultMeasure}}.}

\item{super.learner}{[\code{Learner}]\cr New \code{super.learner} to apply.}

\item{use.feat}{[\code{logical(1)}]\cr Whether the original features should be passed to the super learner.}

\item{parset}{[\code{list}]\cr List containing parameters for \code{hill.climb}. See \code{\link{makeStackedLearner}}.}
}
\description{
Rerun a alreday done outer resample for a StackedLearner again with new settings.
Instead of computing a whole new resampling procedure just use \code{resampleStackedLearnerAgain}. 
\code{resampleStackedLearnerAgain} reuses the already done work in a \code{ResampleResult}, i.e. 
reuse fitted base models (needed for level 1 test data) and reuse level 1 training data. 
Note: This function does not support resample objects with single broken base models (no error 
handling implemented). Moreover models need to present (i.e. save.preds = TRUE in 
\code{makeStackedLearner}). When using \code{save.on.disc = TRUE} in makeStackedLearner 
resampling procedures with holdout are allowed only (model names are not unique 
regarding CV fold number).
This function does four things internally to obtain the new predictions.
\describe{
\item{1.}{Extract level 1 train data.}
\item{2.}{Fit new super learner or apply new ensemble selection setting using level 1 train data.}
\item{3.}{Use saved base models on test data to predict level 1 test data}
\item{4.}{Apply model from (2) on level 1 test data from (3) to obtain final prediction}
}
For method stack.cv \code{super.learner} and \code{use.feat} need to be set. 
For \code{hill.climb} \code{parset} need to be set. 
Method \code{average} is not supported (use no inner resampling).
}
\examples{
tsk = pid.task
bls = list(makeLearner("classif.kknn", id = "k1"), 
  makeLearner("classif.randomForest", id = "f1"),
  makeLearner("classif.rpart", id = "r1", minsplit = 5),
  makeLearner("classif.rpart", id = "r2", minsplit = 10),
  makeLearner("classif.rpart", id = "r3", minsplit = 15),
  makeLearner("classif.rpart", id = "r4", minsplit = 20),
  makeLearner("classif.rpart", id = "r5", minsplit = 25)
)
bls = lapply(bls, function(x) setPredictType(x, predict.type = "prob"))
ste = makeStackedLearner(id = "stack", bls, resampling = cv3, 
  predict.type = "prob", method = "hill.climb", parset = list(init = 1, 
  bagprob = 0.5, bagtime = 3, metric = mmce))
resres = resample(ste, tsk, cv2, models = TRUE) 
re2 = resampleStackedLearnerAgain(obj = resres, task = tsk, parset = list(init = 2))
re3 = resampleStackedLearnerAgain(obj = resres, task = tsk, measures = list(mmce), parset = list(bagprob = .2))
re3 = resampleStackedLearnerAgain(obj = resres, task = tsk, measures = mmce, parset = list(bagprob = .2))
re4 = resampleStackedLearnerAgain(obj = resres, task = tsk, measures = list(acc), parset = list(bagtime = 10))
re5 = resampleStackedLearnerAgain(obj = resres, task = tsk, measures = list(mmce, acc), parset = list(init = 2, bagprob = .7, bagtime = 10))

sapply(list(resres, re2, re3, re4, re5), function(x) x$runtime)
sapply(list(resres, re2, re3, re4, re5), function(x) x$aggr)
}

